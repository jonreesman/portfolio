* Fully connected layer

	* Every neuron in one layer feeds into every neuron in the next layer

	* Feed forward

* **Input layer**

	* First layer of the network

	* Last layer

	* **Output layer** of network

* **Hidden layers**

	* All layers in between

	* Difficult to interpret because of high connectivity and is far away from known inputs and outputs

	* Kind of a black box

* **Deep neural network**

	* Contains two or more hidden layers

	* Layers > 3

* Can approximate any convex continuous function

	* Any function you can continuosly integrate over

	* **Universal Approximation Theorem**

		* Not important to really know but could be cool to see the math

* Above perceptron model isnt entirely honest

	* We need to be able to set constraints to our output values

* **Activation Function**

	* ex. Make all outputs fall between 0 and 1.

	* A very large summation has no upper limit... so we pick a function to apply on the inputs, bias, and weights that constrains output values